{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from IPython.display import FileLink\n",
    "from tqdm import tqdm_notebook as tn #track for loop progress\n",
    "from sklearn.model_selection import train_test_split #split data to train and test\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgbm\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load optimally train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type not in ['object', 'datetime64[ns]']:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train= pd.read_csv('/kaggle/input/train.csv')\n",
    "payment_history = pd.read_csv('/kaggle/input/payment_history.csv')\n",
    "sub = pd.read_csv('/kaggle/input/sample_sub.csv')\n",
    "test = pd.read_csv('/kaggle/input/sample_sub.csv')\n",
    "policy_data = pd.read_csv('/kaggle/input/policy_data.csv')\n",
    "train['Lapse' ] = np.where( ( train.Lapse == \"?\" ) & ( train['Lapse Year'] == \"?\" ), 0,1)\n",
    "train.drop(['Lapse Year'], axis=1, inplace=True)\n",
    "test.drop(['Lapse'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.astype(object)\n",
    "test=test.astype(object)\n",
    "train=reduce_mem_usage(train)\n",
    "test=reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess loaded data (remove outliers,...) & feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lastname(x):\n",
    "    x = str(x)\n",
    "    a = x.split('_')\n",
    "    return a[2]\n",
    "def adresse(x):\n",
    "    x = str(x)\n",
    "    a = x.split('_')\n",
    "    return a[1]\n",
    "def date_from(x):\n",
    "    x=str(x)\n",
    "    a = x.split(' ')\n",
    "    return a[0]\n",
    "def time_from(x):\n",
    "    x=str(x)\n",
    "    a = x.split(' ')\n",
    "    return a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_lim = policy_data['NPR_PREMIUM'].quantile(.95)\n",
    "lower_lim = policy_data['NPR_PREMIUM'].quantile(.05)\n",
    "policy_data.loc[(policy_data['NPR_PREMIUM'] > upper_lim),'NPR_PREMIUM'] = upper_lim\n",
    "policy_data.loc[(policy_data['NPR_PREMIUM'] < lower_lim),'NPR_PREMIUM'] = lower_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_lim = policy_data['NPR_SUMASSURED'].quantile(.95)\n",
    "lower_lim = policy_data['NPR_SUMASSURED'].quantile(.05)\n",
    "policy_data.loc[(policy_data['NPR_SUMASSURED'] > upper_lim),'NPR_SUMASSURED'] = upper_lim\n",
    "policy_data.loc[(policy_data['NPR_SUMASSURED'] < lower_lim),'NPR_SUMASSURED'] = lower_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_lim = policy_data['NLO_AMOUNT'].quantile(.95)\n",
    "lower_lim = policy_data['NLO_AMOUNT'].quantile(.05)\n",
    "policy_data.loc[(policy_data['NLO_AMOUNT'] > upper_lim),'NLO_AMOUNT'] = upper_lim\n",
    "policy_data.loc[(policy_data['NLO_AMOUNT'] < lower_lim),'NLO_AMOUNT'] = lower_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_lim = payment_history['AMOUNTPAID'].quantile(.95)\n",
    "lower_lim = payment_history['AMOUNTPAID'].quantile(.05)\n",
    "payment_history.loc[(payment_history['AMOUNTPAID'] > upper_lim),'AMOUNTPAID'] = upper_lim\n",
    "payment_history.loc[(payment_history['AMOUNTPAID'] < lower_lim),'AMOUNTPAID'] = lower_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_data = policy_data.fillna(policy_data.median())\n",
    "payment_history = payment_history.fillna(policy_data.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_data.rename(columns={'NP2_EFFECTDATE':'EFFECTDATE','PPR_PRODCD':'PRODCD','NPR_PREMIUM':'PREMIUM','NPH_LASTNAME':'LASTNAME','CLF_LIFECD':'LIFECD','NSP_SUBPROPOSAL':'SUBPROPOSAL','NPR_SUMASSURED':'SUMASSURED','NLO_TYPE':'TYPE','NLO_AMOUNT':'AMOUNT'}, inplace=True)\n",
    "policy_data['AGCODE'] = policy_data['AAG_AGCODE'].apply(Lastname)\n",
    "policy_data['LOCATCODE'] = policy_data['PCL_LOCATCODE'].apply(Lastname)\n",
    "policy_data['OCCUPATION_'] = policy_data['OCCUPATION'].apply(adresse)\n",
    "policy_data['CATEGORY_'] = policy_data['CATEGORY'].apply(adresse)\n",
    "policy_data['TYPE_'] = policy_data['TYPE'].apply(adresse)\n",
    "policy_data.drop(['AAG_AGCODE'],axis=1, inplace=True )\n",
    "policy_data.drop(['PCL_LOCATCODE'],axis=1, inplace=True )\n",
    "policy_data.drop(['OCCUPATION'],axis=1, inplace=True )\n",
    "policy_data.drop(['CATEGORY'],axis=1, inplace=True )\n",
    "policy_data.drop(['TYPE'],axis=1, inplace=True )\n",
    "policy_data['EFFECTDATE'] = pd.to_datetime(policy_data['EFFECTDATE'])\n",
    "policy_data['EFFECTDATE_year'] = policy_data['EFFECTDATE'].dt.year\n",
    "policy_data['EFFECTDATE_month'] = policy_data['EFFECTDATE'].dt.month\n",
    "policy_data['EFFECTDATE_day'] = policy_data['EFFECTDATE'].dt.day\n",
    "policy_data['EFFECTDATE_weekday'] = policy_data['EFFECTDATE'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EFFECTDATE_month = pd.get_dummies(policy_data['EFFECTDATE_month'] , prefix='EFFECTDATE_month')\n",
    "policy_data = pd.concat([policy_data,EFFECTDATE_month],axis=1)\n",
    "policy_data.drop(['EFFECTDATE_month'],axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del EFFECTDATE_month\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_data['EFFECTDATE_dayofweek_name']=policy_data['EFFECTDATE'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Category = pd.get_dummies(policy_data['CATEGORY_'] , prefix='Category')\n",
    "policy_data = pd.concat([policy_data,Category],axis=1)\n",
    "policy_data.drop(['CATEGORY_'],axis=1 , inplace = True)\n",
    "Branch_code = pd.get_dummies(policy_data['LOCATCODE'] , prefix='Branch_code')\n",
    "policy_data = pd.concat([policy_data,Branch_code],axis=1)\n",
    "policy_data.drop(['LOCATCODE'],axis=1 , inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_data['total']=policy_data['PREMIUM']+policy_data['AMOUNT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Category , Branch_code\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRODCD = pd.get_dummies(policy_data['PRODCD'] , prefix='PRODCD')\n",
    "policy_data = pd.concat([policy_data,PRODCD],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del PRODCD\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLO_TYPE = pd.get_dummies(policy_data['TYPE_'] , prefix='TYPE')\n",
    "policy_data = pd.concat([policy_data,NLO_TYPE],axis=1)\n",
    "policy_data.drop(['TYPE_'],axis=1 , inplace = True)\n",
    "CLF_LIFECD = pd.get_dummies(policy_data['LIFECD'] , prefix='LIFECD')\n",
    "policy_data = pd.concat([policy_data,CLF_LIFECD],axis=1)\n",
    "policy_data.drop(['LIFECD'],axis=1 , inplace = True)\n",
    "NP2_EFFECTDATE_year = pd.get_dummies(policy_data['EFFECTDATE_year'] , prefix='EFFECTDATE_year')\n",
    "policy_data = pd.concat([policy_data,NP2_EFFECTDATE_year],axis=1)\n",
    "policy_data.drop(['EFFECTDATE_year'],axis=1 , inplace = True)\n",
    "SUBPROPOSAL = pd.get_dummies(policy_data['SUBPROPOSAL'] , prefix='SUBPROPOSAL')\n",
    "policy_data = pd.concat([policy_data,SUBPROPOSAL],axis=1)\n",
    "policy_data.drop(['SUBPROPOSAL'],axis=1 , inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del NLO_TYPE , CLF_LIFECD , NP2_EFFECTDATE_year \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_data['LASTNAME_']=policy_data['LASTNAME'].apply(Lastname)\n",
    "policy_data['PRODCD_']=policy_data['PRODCD'].apply(Lastname)\n",
    "policy_data.drop(['LASTNAME'],axis=1,inplace=True)\n",
    "policy_data.drop(['PRODCD'],axis=1,inplace=True)\n",
    "policy_data.rename(columns={'LASTNAME_':'LASTNAME'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_history['PREMIUMDUEDATE'].fillna(payment_history['PREMIUMDUEDATE'].value_counts().idxmax(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "payment_history['DATEPAID_date'] = payment_history['DATEPAID'].apply(date_from)\n",
    "payment_history['DATEPAID_time'] = payment_history['DATEPAID'].apply(time_from)\n",
    "payment_history['POSTDATE_date'] = payment_history['POSTDATE'].apply(date_from)\n",
    "payment_history['POSTDATE_time'] = payment_history['POSTDATE'].apply(time_from)\n",
    "payment_history['PREMIUMDUEDATE_date'] = payment_history['PREMIUMDUEDATE'].apply(date_from)\n",
    "payment_history['PREMIUMDUEDATE_time'] = payment_history['PREMIUMDUEDATE'].apply(time_from)\n",
    "payment_history['DATEPAID'] = pd.to_datetime(payment_history['DATEPAID'])\n",
    "payment_history['POSTDATE'] = pd.to_datetime(payment_history['POSTDATE'])\n",
    "payment_history['PREMIUMDUEDATE'] = pd.to_datetime(payment_history['PREMIUMDUEDATE'])\n",
    "payment_history['paid_year'] = payment_history['DATEPAID'].dt.year\n",
    "payment_history['paid_month'] = payment_history['DATEPAID'].dt.month\n",
    "payment_history['paid_day'] = payment_history['DATEPAID'].dt.day\n",
    "payment_history['paid_weekday'] = payment_history['DATEPAID'].dt.weekday\n",
    "payment_history['post_year'] = payment_history['POSTDATE'].dt.year\n",
    "payment_history['post_month'] = payment_history['POSTDATE'].dt.month\n",
    "payment_history['post_day'] = payment_history['POSTDATE'].dt.day\n",
    "payment_history['post_weekday'] = payment_history['POSTDATE'].dt.weekday\n",
    "payment_history['premium_year'] = payment_history['PREMIUMDUEDATE'].dt.year\n",
    "payment_history['premium_month'] = payment_history['PREMIUMDUEDATE'].dt.month\n",
    "payment_history['premium_day'] = payment_history['PREMIUMDUEDATE'].dt.day\n",
    "payment_history['premium_weekday'] = payment_history['PREMIUMDUEDATE'].dt.weekday\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premium_year = pd.get_dummies(payment_history['premium_year'] , prefix='premium_year')\n",
    "payment_history = pd.concat([payment_history,premium_year],axis=1)\n",
    "payment_history.drop(['premium_year'],axis=1 , inplace = True)\n",
    "paid_year = pd.get_dummies(payment_history['paid_year'] , prefix='paid_year')\n",
    "payment_history = pd.concat([payment_history,paid_year],axis=1)\n",
    "payment_history.drop(['paid_year'],axis=1 , inplace = True)\n",
    "post_year = pd.get_dummies(payment_history['post_year'] , prefix='post_year')\n",
    "payment_history = pd.concat([payment_history,post_year],axis=1)\n",
    "payment_history.drop(['post_year'],axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del premium_year , paid_year , post_year\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premium_month = pd.get_dummies(payment_history['premium_month'] , prefix='premium_month')\n",
    "payment_history = pd.concat([payment_history,premium_month],axis=1)\n",
    "payment_history.drop(['premium_month'],axis=1 , inplace = True)\n",
    "post_month = pd.get_dummies(payment_history['post_month'] , prefix='post_month')\n",
    "payment_history = pd.concat([payment_history,post_month],axis=1)\n",
    "payment_history.drop(['post_month'],axis=1 , inplace = True)\n",
    "paid_month = pd.get_dummies(payment_history['paid_month'] , prefix='paid_month')\n",
    "payment_history = pd.concat([payment_history,paid_month],axis=1)\n",
    "payment_history.drop(['paid_month'],axis=1 , inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del premium_month , post_month , paid_month \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_history['PREMIUMDUEDATE_dayofweek_name']=payment_history['PREMIUMDUEDATE'].dt.day_name()\n",
    "payment_history['DATEPAID_dayofweek_name']=payment_history['DATEPAID'].dt.day_name()\n",
    "payment_history['POSTDATE_dayofweek_name']=payment_history['POSTDATE'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample train dataset\n",
    "a = policy_data[['LASTNAME','OCCUPATION_']]\n",
    "\n",
    "## Frequency Encoding title variable\n",
    "b = a.groupby(['LASTNAME']).size().reset_index()\n",
    "b.columns = ['LASTNAME', 'Freq_Encoded_OCCUPATION_']\n",
    "policy_data = pd.merge(policy_data,b,on = 'LASTNAME',how = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del a , b \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train = policy_data[['Policy ID','AMOUNT']]\n",
    "## Mean encoding \n",
    "x = sample_train.groupby(['Policy ID'])['AMOUNT'].mean().reset_index()\n",
    "x = x.rename(columns={\"AMOUNT\" : \"AMOUNT\" +\"_Mean_Encoded\"})\n",
    "policy_data = pd.merge(policy_data,x,on = 'Policy ID',how = 'left')\n",
    "policy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sample_train , x \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cat_job(x):\n",
    "    if x>0.07:\n",
    "        return 0 \n",
    "    elif x<0.7 and x>0.01 : \n",
    "        return 1 \n",
    "    elif x<0.01 and x>0.001 : \n",
    "        return 2 \n",
    "    elif x<0.001 and x>0.0001 : \n",
    "        return 3\n",
    "    elif x<0.0001 and x>0.00001 : \n",
    "        return 4\n",
    "    else : \n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_data['OCCUPATION_cat']=np.zeros(policy_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(policy_data['OCCUPATION_'].unique())\n",
    "b={}\n",
    "for elem in a:\n",
    "    b[elem]=policy_data['OCCUPATION_'].value_counts(elem)\n",
    "for i in tn(range(240)):\n",
    "    a=list(b.values())[i][i]\n",
    "    policy_data['OCCUPATION_cat'][i]=set_cat_job(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation = pd.get_dummies(policy_data['OCCUPATION_cat'] , prefix='occupation')\n",
    "policy_data = pd.concat([policy_data,occupation],axis=1)\n",
    "policy_data.drop(['OCCUPATION_cat'],axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_data.drop(['PRODCD_'],axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_data.drop(['AGCODE','OCCUPATION_'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.astype(object)\n",
    "test=test.astype(object)\n",
    "train=reduce_mem_usage(train)\n",
    "test=reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(right=policy_data.reset_index(), how='left', on='Policy ID')\n",
    "train = train.merge(right=payment_history.reset_index(), how='left', on='Policy ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.merge(right=policy_data.reset_index(), how='left', on='Policy ID')\n",
    "test = test.merge(right=payment_history.reset_index(), how='left', on='Policy ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del policy_data , payment_history , occupation , a , b \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop_duplicates(subset =\"Policy ID\", keep ='first', inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna(test.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['PREMIUMDUEDATE_time'].fillna(test['PREMIUMDUEDATE_time'].value_counts().idxmax(), inplace=True)\n",
    "test['PREMIUMDUEDATE_date'].fillna(test['PREMIUMDUEDATE_date'].value_counts().idxmax(), inplace=True)\n",
    "test['POSTDATE_time'].fillna(test['POSTDATE_time'].value_counts().idxmax(), inplace=True)\n",
    "test['POSTDATE_date'].fillna(test['POSTDATE_date'].value_counts().idxmax(), inplace=True)\n",
    "test['DATEPAID_time'].fillna(test['DATEPAID_time'].value_counts().idxmax(), inplace=True)\n",
    "test['DATEPAID_date'].fillna(test['DATEPAID_date'].value_counts().idxmax(), inplace=True)\n",
    "test['PREMIUMDUEDATE'].fillna(test['PREMIUMDUEDATE'].value_counts().idxmax(), inplace=True)\n",
    "test['POSTDATE'].fillna(test['POSTDATE'].value_counts().idxmax(), inplace=True)\n",
    "test['DATEPAID'].fillna(test['DATEPAID'].value_counts().idxmax(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tn(test.columns):\n",
    "    if test[col].dtype != 'object':\n",
    "        test[col]=test[col].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(train.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['index_x'],axis=1,inplace=True)\n",
    "train.drop(['index_y'],axis=1,inplace=True)\n",
    "test.drop(['index_x'],axis=1,inplace=True)\n",
    "test.drop(['index_y'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['PREMIUMDUEDATE_time'].fillna(train['PREMIUMDUEDATE_time'].value_counts().idxmax(), inplace=True)\n",
    "train['PREMIUMDUEDATE_date'].fillna(train['PREMIUMDUEDATE_date'].value_counts().idxmax(), inplace=True)\n",
    "train['POSTDATE_time'].fillna(train['POSTDATE_time'].value_counts().idxmax(), inplace=True)\n",
    "train['POSTDATE_date'].fillna(train['POSTDATE_date'].value_counts().idxmax(), inplace=True)\n",
    "train['DATEPAID_time'].fillna(train['DATEPAID_time'].value_counts().idxmax(), inplace=True)\n",
    "train['DATEPAID_date'].fillna(train['DATEPAID_date'].value_counts().idxmax(), inplace=True)\n",
    "train['PREMIUMDUEDATE'].fillna(train['PREMIUMDUEDATE'].value_counts().idxmax(), inplace=True)\n",
    "train['POSTDATE'].fillna(train['POSTDATE'].value_counts().idxmax(), inplace=True)\n",
    "train['DATEPAID'].fillna(train['DATEPAID'].value_counts().idxmax(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['index'],axis=1,inplace=True)\n",
    "test.drop(['index'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=reduce_mem_usage(test)\n",
    "train=reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tn(train.columns):\n",
    "    if train[f].dtype!='object': \n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(train[f].values))  \n",
    "        train[f] = lbl.transform(list(train[f].values)) \n",
    "for f in tn(test.columns):\n",
    "    if test[f].dtype!='object': \n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(test[f].values))  \n",
    "        test[f] = lbl.transform(list(test[f].values)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train['Lapse']\n",
    "X = train.drop(['Lapse'],axis=1,inplace=False)\n",
    "X=X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data using lgbm algorithm and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective' :'binary',\n",
    "    'learning_rate' : 0.01,\n",
    "    'num_leaves' : 120,\n",
    "    'feature_fraction': 0.4, \n",
    "    'bagging_fraction': 0.4, \n",
    "    'bagging_freq':1,\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'metric': 'binary_logloss',\n",
    "    'max_depth' : -1,\n",
    "    'seed':0,\n",
    "    'lambda_l2':0.4\n",
    "}\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X,Y,  random_state=7, test_size=0.33)\n",
    "    \n",
    "# making lgbm datasets for train and valid\n",
    "d_train = lgbm.Dataset(X_train, Y_train)\n",
    "d_valid = lgbm.Dataset(X_valid, Y_valid)\n",
    "    \n",
    "# training with early stop\n",
    "bst = lgbm.train(params, d_train, 500, valid_sets=[d_valid], verbose_eval=50, early_stopping_rounds=100)\n",
    "    \n",
    "# making prediciton for one column\n",
    "preds = bst.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['Lapse']=preds\n",
    "sub.Lapse.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(submission_file, submission_name):\n",
    "    submission_file.to_csv(submission_name+\".csv\" , index=False)\n",
    "    return FileLink(submission_name+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(sub, 'Zimnat48')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
